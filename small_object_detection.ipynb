{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1WesKiy_QZq2_UbXo6jqIpGztlUAbkHmV","authorship_tag":"ABX9TyMPXMuXmDTPAGsxtLH6ojzC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vXr71yIR_tFA"},"outputs":[],"source":["!wget --load-cookies ~/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies ~/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1aXTRM0tBRYvwHay3nqK0F4Huo0xhCafT' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1aXTRM0tBRYvwHay3nqK0F4Huo0xhCafT\" -O data.zip && rm -rf ~/cookies.txt\n","!wget --load-cookies ~/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies ~/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1kLkFFx_9zx42os_0WXCOM4-hM-Wetq6R' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1kLkFFx_9zx42os_0WXCOM4-hM-Wetq6R\" -O config.zip && rm -rf ~/cookies.txt"]},{"cell_type":"code","source":["!unzip data.zip\n","!unzip config.zip"],"metadata":{"id":"qvCwzv3LAUsI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip3 install terminaltables"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BBWYzIcuJ6dL","executionInfo":{"status":"ok","timestamp":1666718360676,"user_tz":-540,"elapsed":3949,"user":{"displayName":"‍최인훈[학생](소프트웨어융합대학 컴퓨터공학과)","userId":"02055695527872939247"}},"outputId":"6775e906-96af-4140-9d66-359a7bf61238"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting terminaltables\n","  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n","Installing collected packages: terminaltables\n","Successfully installed terminaltables-3.1.10\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","import torchvision.transforms as transforms\n","\n","import datetime\n","import argparse\n","import warnings\n","import glob\n","import random\n","import os\n","import sys\n","import math\n","import time\n","import tqdm\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","\n","from terminaltables import AsciiTable\n","from PIL import Image"],"metadata":{"id":"rvIQGfa_J4PI","executionInfo":{"status":"ok","timestamp":1666718362488,"user_tz":-540,"elapsed":1,"user":{"displayName":"‍최인훈[학생](소프트웨어융합대학 컴퓨터공학과)","userId":"02055695527872939247"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Parse config function\n","\n","def parse_model_config(path):\n","    \"\"\"Parses the yolo-v3 layer configuration file and returns module definitions\"\"\"\n","    file = open(path, 'r')\n","    lines = file.read().split('\\n')\n","    lines = [x for x in lines if x and not x.startswith('#')]\n","    lines = [x.rstrip().lstrip() for x in lines] # get rid of fringe whitespaces\n","    module_defs = []\n","    for line in lines:\n","        if line.startswith('['): # This marks the start of a new block\n","            module_defs.append({})\n","            module_defs[-1]['type'] = line[1:-1].rstrip()\n","            if module_defs[-1]['type'] == 'convolutional':\n","                module_defs[-1]['batch_normalize'] = 0\n","        else:\n","            key, value = line.split(\"=\")\n","            value = value.strip()\n","            module_defs[-1][key.rstrip()] = value.strip()\n","\n","    return module_defs\n","\n","def parse_data_config(path):\n","    \"\"\"Parses the data configuration file\"\"\"\n","    options = dict()\n","    options['gpus'] = '0,1,2,3'\n","    options['num_workers'] = '10'\n","    with open(path, 'r') as fp:\n","        lines = fp.readlines()\n","    for line in lines:\n","        line = line.strip()\n","        if line == '' or line.startswith('#'):\n","            continue\n","        key, value = line.split('=')\n","        options[key.strip()] = value.strip()\n","    return options\n"],"metadata":{"id":"WS0h2Vf3FoWj","executionInfo":{"status":"ok","timestamp":1666718004932,"user_tz":-540,"elapsed":272,"user":{"displayName":"‍최인훈[학생](소프트웨어융합대학 컴퓨터공학과)","userId":"02055695527872939247"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# Utils\n","\n","def to_cpu(tensor):\n","    return tensor.detach().cpu()\n","\n","\n","def load_classes(path):\n","    \"\"\"\n","    Loads class labels at 'path'\n","    \"\"\"\n","    fp = open(path, \"r\")\n","    names = fp.read().split(\"\\n\")[:-1]\n","    return names\n","\n","\n","def weights_init_normal(m):\n","    classname = m.__class__.__name__\n","    if classname.find(\"Conv\") != -1:\n","        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find(\"BatchNorm2d\") != -1:\n","        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        torch.nn.init.constant_(m.bias.data, 0.0)\n","\n","\n","def rescale_boxes(boxes, current_dim, original_shape):\n","    \"\"\" Rescales bounding boxes to the original shape \"\"\"\n","    orig_h, orig_w = original_shape\n","    # The amount of padding that was added\n","    pad_x = max(orig_h - orig_w, 0) * (current_dim / max(original_shape))\n","    pad_y = max(orig_w - orig_h, 0) * (current_dim / max(original_shape))\n","    # Image height and width after padding is removed\n","    unpad_h = current_dim - pad_y\n","    unpad_w = current_dim - pad_x\n","    # Rescale bounding boxes to dimension of original image\n","    boxes[:, 0] = ((boxes[:, 0] - pad_x // 2) / unpad_w) * orig_w\n","    boxes[:, 1] = ((boxes[:, 1] - pad_y // 2) / unpad_h) * orig_h\n","    boxes[:, 2] = ((boxes[:, 2] - pad_x // 2) / unpad_w) * orig_w\n","    boxes[:, 3] = ((boxes[:, 3] - pad_y // 2) / unpad_h) * orig_h\n","    return boxes\n","\n","\n","def xywh2xyxy(x):\n","    y = x.new(x.shape)\n","    y[..., 0] = x[..., 0] - x[..., 2] / 2\n","    y[..., 1] = x[..., 1] - x[..., 3] / 2\n","    y[..., 2] = x[..., 0] + x[..., 2] / 2\n","    y[..., 3] = x[..., 1] + x[..., 3] / 2\n","    return y\n","\n","\n","def ap_per_class(tp, conf, pred_cls, target_cls):\n","    \"\"\" Compute the average precision, given the recall and precision curves.\n","    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n","    # Arguments\n","        tp:    True positives (list).\n","        conf:  Objectness value from 0-1 (list).\n","        pred_cls: Predicted object classes (list).\n","        target_cls: True object classes (list).\n","    # Returns\n","        The average precision as computed in py-faster-rcnn.\n","    \"\"\"\n","\n","    # Sort by objectness\n","    i = np.argsort(-conf)\n","    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n","\n","    # Find unique classes\n","    unique_classes = np.unique(target_cls)\n","\n","    # Create Precision-Recall curve and compute AP for each class\n","    ap, p, r = [], [], []\n","    for c in tqdm.tqdm(unique_classes, desc=\"Computing AP\"):\n","        i = pred_cls == c\n","        n_gt = (target_cls == c).sum()  # Number of ground truth objects\n","        n_p = i.sum()  # Number of predicted objects\n","\n","        if n_p == 0 and n_gt == 0:\n","            continue\n","        elif n_p == 0 or n_gt == 0:\n","            ap.append(0)\n","            r.append(0)\n","            p.append(0)\n","        else:\n","            # Accumulate FPs and TPs\n","            fpc = (1 - tp[i]).cumsum()\n","            tpc = (tp[i]).cumsum()\n","\n","            # Recall\n","            recall_curve = tpc / (n_gt + 1e-16)\n","            r.append(recall_curve[-1])\n","\n","            # Precision\n","            precision_curve = tpc / (tpc + fpc)\n","            p.append(precision_curve[-1])\n","\n","            # AP from recall-precision curve\n","            ap.append(compute_ap(recall_curve, precision_curve))\n","\n","    # Compute F1 score (harmonic mean of precision and recall)\n","    p, r, ap = np.array(p), np.array(r), np.array(ap)\n","    f1 = 2 * p * r / (p + r + 1e-16)\n","\n","    return p, r, ap, f1, unique_classes.astype(\"int32\")\n","\n","\n","def compute_ap(recall, precision):\n","    \"\"\" Compute the average precision, given the recall and precision curves.\n","    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n","\n","    # Arguments\n","        recall:    The recall curve (list).\n","        precision: The precision curve (list).\n","    # Returns\n","        The average precision as computed in py-faster-rcnn.\n","    \"\"\"\n","    # correct AP calculation\n","    # first append sentinel values at the end\n","    mrec = np.concatenate(([0.0], recall, [1.0]))\n","    mpre = np.concatenate(([0.0], precision, [0.0]))\n","\n","    # compute the precision envelope\n","    for i in range(mpre.size - 1, 0, -1):\n","        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n","\n","    # to calculate area under PR curve, look for points\n","    # where X axis (recall) changes value\n","    i = np.where(mrec[1:] != mrec[:-1])[0]\n","\n","    # and sum (\\Delta recall) * prec\n","    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n","    return ap\n","\n","\n","def get_batch_statistics(outputs, targets, iou_threshold):\n","    \"\"\" Compute true positives, predicted scores and predicted labels per sample \"\"\"\n","    batch_metrics = []\n","    for sample_i in range(len(outputs)):\n","\n","        if outputs[sample_i] is None:\n","            continue\n","\n","        output = outputs[sample_i]\n","        pred_boxes = output[:, :4]\n","        pred_scores = output[:, 4]\n","        pred_labels = output[:, -1]\n","\n","        true_positives = np.zeros(pred_boxes.shape[0])\n","\n","        annotations = targets[targets[:, 0] == sample_i][:, 1:]\n","        target_labels = annotations[:, 0] if len(annotations) else []\n","        if len(annotations):\n","            detected_boxes = []\n","            target_boxes = annotations[:, 1:]\n","\n","            for pred_i, (pred_box, pred_label) in enumerate(zip(pred_boxes, pred_labels)):\n","\n","                # If targets are found break\n","                if len(detected_boxes) == len(annotations):\n","                    break\n","\n","                # Ignore if label is not one of the target labels\n","                if pred_label not in target_labels:\n","                    continue\n","\n","                iou, box_index = bbox_iou(pred_box.unsqueeze(0), target_boxes).max(0)\n","                if iou >= iou_threshold and box_index not in detected_boxes:\n","                    true_positives[pred_i] = 1\n","                    detected_boxes += [box_index]\n","        batch_metrics.append([true_positives, pred_scores, pred_labels])\n","    return batch_metrics\n","\n","\n","def bbox_wh_iou(wh1, wh2):\n","    wh2 = wh2.t()\n","    w1, h1 = wh1[0], wh1[1]\n","    w2, h2 = wh2[0], wh2[1]\n","    inter_area = torch.min(w1, w2) * torch.min(h1, h2)\n","    union_area = (w1 * h1 + 1e-16) + w2 * h2 - inter_area\n","    return inter_area / union_area\n","\n","\n","def bbox_iou(box1, box2, x1y1x2y2=True):\n","    \"\"\"\n","    Returns the IoU of two bounding boxes\n","    \"\"\"\n","    if not x1y1x2y2:\n","        # Transform from center and width to exact coordinates\n","        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n","        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n","        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n","        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n","    else:\n","        # Get the coordinates of bounding boxes\n","        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n","        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n","\n","    # get the corrdinates of the intersection rectangle\n","    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n","    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n","    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n","    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n","    # Intersection area\n","    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(\n","        inter_rect_y2 - inter_rect_y1 + 1, min=0\n","    )\n","    # Union Area\n","    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n","    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n","\n","    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n","\n","    return iou\n","\n","\n","def non_max_suppression(prediction, conf_thres=0.5, nms_thres=0.4):\n","    \"\"\"\n","    Removes detections with lower object confidence score than 'conf_thres' and performs\n","    Non-Maximum Suppression to further filter detections.\n","    Returns detections with shape:\n","        (x1, y1, x2, y2, object_conf, class_score, class_pred)\n","    \"\"\"\n","\n","    # From (center x, center y, width, height) to (x1, y1, x2, y2)\n","    prediction[..., :4] = xywh2xyxy(prediction[..., :4])\n","    output = [None for _ in range(len(prediction))]\n","    for image_i, image_pred in enumerate(prediction):\n","        # Filter out confidence scores below threshold\n","        image_pred = image_pred[image_pred[:, 4] >= conf_thres]\n","        # If none are remaining => process next image\n","        if not image_pred.size(0):\n","            continue\n","        # Object confidence times class confidence\n","        score = image_pred[:, 4] * image_pred[:, 5:].max(1)[0]\n","        # Sort by it\n","        image_pred = image_pred[(-score).argsort()]\n","        class_confs, class_preds = image_pred[:, 5:].max(1, keepdim=True)\n","        detections = torch.cat((image_pred[:, :5], class_confs.float(), class_preds.float()), 1)\n","        # Perform non-maximum suppression\n","        keep_boxes = []\n","        while detections.size(0):\n","            large_overlap = bbox_iou(detections[0, :4].unsqueeze(0), detections[:, :4]) > nms_thres\n","            label_match = detections[0, -1] == detections[:, -1]\n","            # Indices of boxes with lower confidence scores, large IOUs and matching labels\n","            invalid = large_overlap & label_match\n","            weights = detections[invalid, 4:5]\n","            # Merge overlapping bboxes by order of confidence\n","            detections[0, :4] = (weights * detections[invalid, :4]).sum(0) / weights.sum()\n","            keep_boxes += [detections[0]]\n","            detections = detections[~invalid]\n","        if keep_boxes:\n","            output[image_i] = torch.stack(keep_boxes)\n","\n","    return output\n","\n","\n","def build_targets(pred_boxes, pred_cls, target, anchors, ignore_thres):\n","\n","    ByteTensor = torch.cuda.ByteTensor if pred_boxes.is_cuda else torch.ByteTensor\n","    FloatTensor = torch.cuda.FloatTensor if pred_boxes.is_cuda else torch.FloatTensor\n","\n","    nB = pred_boxes.size(0)\n","    nA = pred_boxes.size(1)\n","    nC = pred_cls.size(-1)\n","    nG = pred_boxes.size(2)\n","\n","    # Output tensors\n","    obj_mask = ByteTensor(nB, nA, nG, nG).fill_(0)\n","    noobj_mask = ByteTensor(nB, nA, nG, nG).fill_(1)\n","    class_mask = FloatTensor(nB, nA, nG, nG).fill_(0)\n","    iou_scores = FloatTensor(nB, nA, nG, nG).fill_(0)\n","    tx = FloatTensor(nB, nA, nG, nG).fill_(0)\n","    ty = FloatTensor(nB, nA, nG, nG).fill_(0)\n","    tw = FloatTensor(nB, nA, nG, nG).fill_(0)\n","    th = FloatTensor(nB, nA, nG, nG).fill_(0)\n","    tcls = FloatTensor(nB, nA, nG, nG, nC).fill_(0)\n","\n","    # Convert to position relative to box\n","    target_boxes = target[:, 2:6] * nG\n","    gxy = target_boxes[:, :2]\n","    gwh = target_boxes[:, 2:]\n","    # Get anchors with best iou\n","    ious = torch.stack([bbox_wh_iou(anchor, gwh) for anchor in anchors])\n","    best_ious, best_n = ious.max(0)\n","    # Separate target values\n","    b, target_labels = target[:, :2].long().t()\n","    gx, gy = gxy.t()\n","    gw, gh = gwh.t()\n","    gi, gj = gxy.long().t()\n","    # Set masks\n","    obj_mask[b, best_n, gj, gi] = 1\n","    noobj_mask[b, best_n, gj, gi] = 0\n","\n","    # Set noobj mask to zero where iou exceeds ignore threshold\n","    for i, anchor_ious in enumerate(ious.t()):\n","        noobj_mask[b[i], anchor_ious > ignore_thres, gj[i], gi[i]] = 0\n","\n","    # Coordinates\n","    tx[b, best_n, gj, gi] = gx - gx.floor()\n","    ty[b, best_n, gj, gi] = gy - gy.floor()\n","    # Width and height\n","    tw[b, best_n, gj, gi] = torch.log(gw / anchors[best_n][:, 0] + 1e-16)\n","    th[b, best_n, gj, gi] = torch.log(gh / anchors[best_n][:, 1] + 1e-16)\n","    # One-hot encoding of label\n","    tcls[b, best_n, gj, gi, target_labels] = 1\n","    # Compute label correctness and iou at best anchor\n","    class_mask[b, best_n, gj, gi] = (pred_cls[b, best_n, gj, gi].argmax(-1) == target_labels).float()\n","    iou_scores[b, best_n, gj, gi] = bbox_iou(pred_boxes[b, best_n, gj, gi], target_boxes, x1y1x2y2=False)\n","\n","    tconf = obj_mask.float()\n","    return iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","def get_scale(detections):\n","    num_roi = detections.size(0)\n","    outputs = []\n","    for num in range(num_roi):\n","        x1, y1, x2, y2 = detections[num]\n","        standard = 416/7\n","        x1_scale = math.floor(x1/standard)\n","        y1_scale = math.floor(y1/standard)\n","        x2_scale = math.ceil(x2/standard)\n","        y2_scale = math.ceil(y2/standard)\n","        outputs.append([x1_scale, y1_scale, x2_scale, y2_scale])\n","    \n","    return outputs\n"],"metadata":{"id":"Kz7RLvXnIojw","executionInfo":{"status":"ok","timestamp":1666718393069,"user_tz":-540,"elapsed":251,"user":{"displayName":"‍최인훈[학생](소프트웨어융합대학 컴퓨터공학과)","userId":"02055695527872939247"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# Dataset\n","\n","def pad_to_square(img, pad_value):\n","    c, h, w = img.shape\n","    dim_diff = np.abs(h - w)\n","    # (upper / left) padding and (lower / right) padding\n","    pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n","    # Determine padding\n","    pad = (0, 0, pad1, pad2) if h <= w else (pad1, pad2, 0, 0)\n","    # Add padding\n","    img = F.pad(img, pad, \"constant\", value=pad_value)\n","\n","    return img, pad\n","\n","\n","def resize(image, size):\n","    image = F.interpolate(image.unsqueeze(0), size=size, mode=\"nearest\").squeeze(0)\n","    return image\n","\n","\n","def random_resize(images, min_size=288, max_size=448):\n","    new_size = random.sample(list(range(min_size, max_size + 1, 32)), 1)[0]\n","    images = F.interpolate(images, size=new_size, mode=\"nearest\")\n","    return images\n","\n","\n","class ImageFolder(Dataset):\n","    def __init__(self, folder_path, img_size=416):\n","        self.files = sorted(glob.glob(\"%s/*.*\" % folder_path))\n","        self.img_size = img_size\n","\n","    def __getitem__(self, index):\n","        img_path = self.files[index % len(self.files)]\n","        # Extract image as PyTorch tensor\n","        img = transforms.ToTensor()(Image.open(img_path))\n","        # Pad to square resolution\n","        img, _ = pad_to_square(img, 0)\n","        # Resize\n","        img = resize(img, self.img_size)\n","\n","        return img_path, img\n","\n","    def __len__(self):\n","        return len(self.files)\n","\n","\n","class ListDataset(Dataset):\n","    def __init__(self, list_path, img_size=416, augment=True, multiscale=True, normalized_labels=True):\n","        with open(list_path, \"r\") as file:\n","            self.img_files = file.readlines()\n","\n","        self.label_files = [\n","            path.replace(\"images\", \"labels\").replace(\".png\", \".txt\").replace(\".jpg\", \".txt\")\n","            for path in self.img_files\n","        ]\n","        self.img_size = img_size\n","        self.max_objects = 100\n","        self.augment = augment\n","        self.multiscale = multiscale\n","        self.normalized_labels = normalized_labels\n","        self.min_size = self.img_size - 3 * 32\n","        self.max_size = self.img_size + 3 * 32\n","        self.batch_count = 0\n","\n","    def __getitem__(self, index):\n","\n","        # ---------\n","        #  Image\n","        # ---------\n","\n","        img_path = self.img_files[index % len(self.img_files)].rstrip()\n","        # Extract image as PyTorch tensor\n","        img = transforms.ToTensor()(Image.open(img_path, 'r').convert('RGB'))\n","\n","        # Handle images with less than three channels\n","        if len(img.shape) != 3:\n","            img = img.unsqueeze(0)\n","            img = img.expand((3, img.shape[1:]))\n","\n","        _, h, w = img.shape\n","        h_factor, w_factor = (h, w) if self.normalized_labels else (1, 1)\n","        # Pad to square resolution\n","        img, pad = pad_to_square(img, 0)\n","        _, padded_h, padded_w = img.shape\n","\n","        # ---------\n","        #  Label\n","        # ---------\n","\n","        label_path = self.label_files[index % len(self.img_files)].rstrip()\n","\n","        targets = None\n","        if os.path.exists(label_path):\n","            boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n","            # Extract coordinates for unpadded + unscaled image\n","            x1 = w_factor * (boxes[:, 1] - boxes[:, 3] / 2)\n","            y1 = h_factor * (boxes[:, 2] - boxes[:, 4] / 2)\n","            x2 = w_factor * (boxes[:, 1] + boxes[:, 3] / 2)\n","            y2 = h_factor * (boxes[:, 2] + boxes[:, 4] / 2)\n","            # Adjust for added padding\n","            x1 += pad[0]\n","            y1 += pad[2]\n","            x2 += pad[1]\n","            y2 += pad[3]\n","            # Returns (x, y, w, h)\n","            boxes[:, 1] = ((x1 + x2) / 2) / padded_w\n","            boxes[:, 2] = ((y1 + y2) / 2) / padded_h\n","            boxes[:, 3] *= w_factor / padded_w\n","            boxes[:, 4] *= h_factor / padded_h\n","\n","            targets = torch.zeros((len(boxes), 6))\n","            targets[:, 1:] = boxes\n","\n","        return img_path, img, targets\n","\n","    def collate_fn(self, batch):\n","        paths, imgs, targets = list(zip(*batch))\n","        # Remove empty placeholder targets\n","        targets = [boxes for boxes in targets if boxes is not None]\n","        # Add sample index to targets\n","        for i, boxes in enumerate(targets):\n","            boxes[:, 0] = i\n","        targets = torch.cat(targets, 0)\n","        # Selects new image size every tenth batch\n","        if self.multiscale and self.batch_count % 10 == 0:\n","            self.img_size = random.choice(range(self.min_size, self.max_size + 1, 32))\n","        # Resize images to input shape\n","        imgs = torch.stack([resize(img, self.img_size) for img in imgs])\n","        self.batch_count += 1\n","        return paths, imgs, targets\n","\n","    def __len__(self):\n","        return len(self.img_files)"],"metadata":{"id":"bJ40opvdKGsJ","executionInfo":{"status":"ok","timestamp":1666718417310,"user_tz":-540,"elapsed":269,"user":{"displayName":"‍최인훈[학생](소프트웨어융합대학 컴퓨터공학과)","userId":"02055695527872939247"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Model\n","\n","def create_modules(module_defs):\n","    \"\"\"\n","    Constructs module list of layer blocks from module configuration in module_defs\n","    \"\"\"\n","    hyperparams = module_defs.pop(0)\n","    output_filters = [int(hyperparams[\"channels\"])]\n","    module_list = nn.ModuleList()\n","    for module_i, module_def in enumerate(module_defs):\n","        modules = nn.Sequential()\n","\n","        if module_def[\"type\"] == \"convolutional\":\n","            bn = int(module_def[\"batch_normalize\"])\n","            filters = int(module_def[\"filters\"])\n","            kernel_size = int(module_def[\"size\"])\n","            pad = (kernel_size - 1) // 2\n","            modules.add_module(\n","                \"conv_{}\".format(module_i),\n","                nn.Conv2d(\n","                    in_channels=output_filters[-1],\n","                    out_channels=filters,\n","                    kernel_size=kernel_size,\n","                    stride=int(module_def[\"stride\"]),\n","                    padding=pad,\n","                    bias=not bn,\n","                ),\n","            )\n","            if bn:\n","                modules.add_module(\"batch_norm_{}\".format(module_i), nn.BatchNorm2d(filters, momentum=0.9, eps=1e-5))\n","            if module_def[\"activation\"] == \"leaky\":\n","                modules.add_module(\"leaky_{}\".format(module_i), nn.LeakyReLU(0.1))\n","\n","        elif module_def[\"type\"] == \"maxpool\":\n","            kernel_size = int(module_def[\"size\"])\n","            stride = int(module_def[\"stride\"])\n","            if kernel_size == 2 and stride == 1:\n","                modules.add_module(\"_debug_padding_{}\".format(module_i), nn.ZeroPad2d((0, 1, 0, 1)))\n","            maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=int((kernel_size - 1) // 2))\n","            modules.add_module(\"maxpool_{}\".format(module_i), maxpool)\n","\n","        elif module_def[\"type\"] == \"upsample\":\n","            upsample = Upsample(scale_factor=int(module_def[\"stride\"]), mode=\"nearest\")\n","            modules.add_module(\"upsample_{}\".format(module_i), upsample)\n","\n","        elif module_def[\"type\"] == \"route\":\n","            layers = [int(x) for x in module_def[\"layers\"].split(\",\")]\n","            filters = sum([output_filters[1:][i] for i in layers])\n","            modules.add_module(\"route_{}\".format(module_i), EmptyLayer())\n","\n","        elif module_def[\"type\"] == \"shortcut\":  # 없음\n","            filters = output_filters[1:][int(module_def[\"from\"])]\n","            modules.add_module(\"shortcut_{}\".format(module_i), EmptyLayer())\n","\n","        elif module_def[\"type\"] == \"yolo\":\n","            anchor_idxs = [int(x) for x in module_def[\"mask\"].split(\",\")]\n","            # Extract anchors\n","            anchors = [int(x) for x in module_def[\"anchors\"].split(\",\")]\n","            anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]\n","            anchors = [anchors[i] for i in anchor_idxs]\n","            num_classes = int(module_def[\"classes\"])\n","            img_size = int(hyperparams[\"height\"])\n","            # Define detection layer\n","            yolo_layer = YOLOLayer(anchors, num_classes, img_size)\n","            modules.add_module(\"yolo_{}\".format(module_i), yolo_layer)\n","        # Register module list and number of output filters\n","        module_list.append(modules)\n","        output_filters.append(filters)\n","\n","    return hyperparams, module_list\n","\n","\n","class Upsample(nn.Module):\n","    \"\"\" nn.Upsample is deprecated \"\"\"\n","\n","    def __init__(self, scale_factor, mode=\"nearest\"):\n","        super(Upsample, self).__init__()\n","        self.scale_factor = scale_factor\n","        self.mode = mode\n","\n","    def forward(self, x):\n","        x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n","        return x\n","\n","\n","class EmptyLayer(nn.Module):\n","    \"\"\"Placeholder for 'route' and 'shortcut' layers\"\"\"\n","\n","    def __init__(self):\n","        super(EmptyLayer, self).__init__()\n","\n","\n","class YOLOLayer(nn.Module):\n","    \"\"\"Detection layer\"\"\"\n","\n","    def __init__(self, anchors, num_classes, img_dim=416):\n","        super(YOLOLayer, self).__init__()\n","        self.anchors = anchors\n","        self.num_anchors = len(anchors)\n","        self.num_classes = num_classes\n","        self.ignore_thres = 0.5\n","        self.mse_loss = nn.MSELoss()\n","        self.bce_loss = nn.BCELoss()\n","        self.obj_scale = 1\n","        self.noobj_scale = 100\n","        self.metrics = {}\n","        self.img_dim = img_dim\n","        self.grid_size = 0  # grid size\n","\n","    def compute_grid_offsets(self, grid_size, cuda=True):\n","        self.grid_size = grid_size\n","        g = self.grid_size\n","        FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","        self.stride = self.img_dim / self.grid_size\n","        # Calculate offsets for each grid\n","        self.grid_x = torch.arange(g).repeat(g, 1).view([1, 1, g, g]).type(FloatTensor)\n","        self.grid_y = torch.arange(g).repeat(g, 1).t().view([1, 1, g, g]).type(FloatTensor)\n","        self.scaled_anchors = FloatTensor([(a_w / self.stride, a_h / self.stride) for a_w, a_h in self.anchors])\n","        self.anchor_w = self.scaled_anchors[:, 0:1].view((1, self.num_anchors, 1, 1))\n","        self.anchor_h = self.scaled_anchors[:, 1:2].view((1, self.num_anchors, 1, 1))\n","\n","    def forward(self, x, targets=None, img_dim=None):\n","\n","        # Tensors for cuda support\n","        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n","        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n","        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor\n","\n","        self.img_dim = img_dim\n","        num_samples = x.size(0)\n","        grid_size = x.size(2)\n","\n","        prediction = (\n","            x.view(num_samples, self.num_anchors, self.num_classes + 5, grid_size, grid_size)\n","            .permute(0, 1, 3, 4, 2)\n","            .contiguous()\n","        )\n","\n","        # Get outputs\n","        x = torch.sigmoid(prediction[..., 0])  # Center x\n","        y = torch.sigmoid(prediction[..., 1])  # Center y\n","        w = prediction[..., 2]  # Width\n","        h = prediction[..., 3]  # Height\n","        pred_conf = torch.sigmoid(prediction[..., 4])  # Conf\n","        pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.\n","\n","        # If grid size does not match current we compute new offsets\n","        if grid_size != self.grid_size:\n","            self.compute_grid_offsets(grid_size, cuda=x.is_cuda)\n","\n","        # Add offset and scale with anchors\n","        pred_boxes = FloatTensor(prediction[..., :4].shape)\n","        pred_boxes[..., 0] = x.data + self.grid_x\n","        pred_boxes[..., 1] = y.data + self.grid_y\n","        pred_boxes[..., 2] = torch.exp(w.data) * self.anchor_w\n","        pred_boxes[..., 3] = torch.exp(h.data) * self.anchor_h\n","\n","        output = torch.cat(\n","            (\n","                pred_boxes.view(num_samples, -1, 4) * self.stride,\n","                pred_conf.view(num_samples, -1, 1),\n","                pred_cls.view(num_samples, -1, self.num_classes),\n","            ),\n","            -1,\n","        )\n","\n","        if targets is None:\n","            return output, 0\n","        else:\n","            iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf = build_targets(\n","                pred_boxes=pred_boxes,\n","                pred_cls=pred_cls,\n","                target=targets,\n","                anchors=self.scaled_anchors,\n","                ignore_thres=self.ignore_thres,\n","            )\n","\n","            # Loss : Mask outputs to ignore non-existing objects (except with conf. loss)\n","            loss_x = self.mse_loss(x[obj_mask], tx[obj_mask])\n","            loss_y = self.mse_loss(y[obj_mask], ty[obj_mask])\n","            loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])\n","            loss_h = self.mse_loss(h[obj_mask], th[obj_mask])\n","            loss_conf_obj = self.bce_loss(pred_conf[obj_mask], tconf[obj_mask])\n","            loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])\n","            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj\n","            loss_cls = self.bce_loss(pred_cls[obj_mask], tcls[obj_mask])\n","            total_loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls\n","\n","            # Metrics\n","            cls_acc = 100 * class_mask[obj_mask].mean()\n","            conf_obj = pred_conf[obj_mask].mean()\n","            conf_noobj = pred_conf[noobj_mask].mean()\n","            conf50 = (pred_conf > 0.5).float()\n","            iou50 = (iou_scores > 0.5).float()\n","            iou75 = (iou_scores > 0.75).float()\n","            detected_mask = conf50 * class_mask * tconf\n","            precision = torch.sum(iou50 * detected_mask) / (conf50.sum() + 1e-16)\n","            recall50 = torch.sum(iou50 * detected_mask) / (obj_mask.sum() + 1e-16)\n","            recall75 = torch.sum(iou75 * detected_mask) / (obj_mask.sum() + 1e-16)\n","\n","            self.metrics = {\n","                \"loss\": to_cpu(total_loss).item(),\n","                \"x\": to_cpu(loss_x).item(),\n","                \"y\": to_cpu(loss_y).item(),\n","                \"w\": to_cpu(loss_w).item(),\n","                \"h\": to_cpu(loss_h).item(),\n","                \"conf\": to_cpu(loss_conf).item(),\n","                \"cls\": to_cpu(loss_cls).item(),\n","                \"cls_acc\": to_cpu(cls_acc).item(),\n","                \"recall50\": to_cpu(recall50).item(),\n","                \"recall75\": to_cpu(recall75).item(),\n","                \"precision\": to_cpu(precision).item(),\n","                \"conf_obj\": to_cpu(conf_obj).item(),\n","                \"conf_noobj\": to_cpu(conf_noobj).item(),\n","                \"grid_size\": grid_size,\n","            }\n","\n","            return output, total_loss\n","\n","\n","class Darknet(nn.Module):\n","    \"\"\"YOLOv3 object detection model\"\"\"\n","\n","    def __init__(self, config_path, img_size=416):\n","        super(Darknet, self).__init__()\n","        self.module_defs = parse_model_config(config_path)\n","        self.hyperparams, self.module_list = create_modules(self.module_defs)\n","        self.yolo_layers = [layer[0] for layer in self.module_list if hasattr(layer[0], \"metrics\")]\n","        self.img_size = img_size\n","        self.seen = 0\n","        self.header_info = np.array([0, 0, 0, self.seen, 0], dtype=np.int32)\n","\n","    def forward(self, x, targets=None):\n","        img_dim = x.shape[2]\n","        loss = 0\n","        layer_outputs, yolo_outputs = [], []\n","        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n","            if module_def[\"type\"] in [\"convolutional\", \"upsample\", \"maxpool\"]:\n","                x = module(x)\n","            elif module_def[\"type\"] == \"route\":\n","                x = torch.cat([layer_outputs[int(layer_i)] for layer_i in module_def[\"layers\"].split(\",\")], 1)\n","            elif module_def[\"type\"] == \"shortcut\": # shortcut없음\n","                layer_i = int(module_def[\"from\"])\n","                x = layer_outputs[-1] + layer_outputs[layer_i]\n","            elif module_def[\"type\"] == \"yolo\":\n","                x, layer_loss = module[0](x, targets, img_dim)\n","                loss += layer_loss\n","                yolo_outputs.append(x)\n","            layer_outputs.append(x)\n","        yolo_outputs = to_cpu(torch.cat(yolo_outputs, 1))\n","        return yolo_outputs if targets is None else (loss, yolo_outputs)\n","\n","    def load_darknet_weights(self, weights_path):\n","        \"\"\"Parses and loads the weights stored in 'weights_path'\"\"\"\n","\n","        # Open the weights file\n","        with open(weights_path, \"rb\") as f:\n","            header = np.fromfile(f, dtype=np.int32, count=5)  # First five are header values\n","            self.header_info = header  # Needed to write header when saving weights\n","            self.seen = header[3]  # number of images seen during training\n","            weights = np.fromfile(f, dtype=np.float32)  # The rest are weights\n","\n","        # Establish cutoff for loading backbone weights\n","        cutoff = None\n","        if \"darknet53.conv.74\" in weights_path:\n","            cutoff = 75\n","\n","        ptr = 0\n","        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n","            if i == cutoff:\n","                break\n","            if module_def[\"type\"] == \"convolutional\":\n","                conv_layer = module[0]\n","                if module_def[\"batch_normalize\"]:\n","                    # Load BN bias, weights, running mean and running variance\n","                    bn_layer = module[1]\n","                    num_b = bn_layer.bias.numel()  # Number of biases\n","                    # Bias\n","                    bn_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.bias)\n","                    bn_layer.bias.data.copy_(bn_b)\n","                    ptr += num_b\n","                    # Weight\n","                    bn_w = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.weight)\n","                    bn_layer.weight.data.copy_(bn_w)\n","                    ptr += num_b\n","                    # Running Mean\n","                    bn_rm = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_mean)\n","                    bn_layer.running_mean.data.copy_(bn_rm)\n","                    ptr += num_b\n","                    # Running Var\n","                    bn_rv = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_var)\n","                    bn_layer.running_var.data.copy_(bn_rv)\n","                    ptr += num_b\n","                else:\n","                    # Load conv. bias\n","                    num_b = conv_layer.bias.numel()\n","                    conv_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(conv_layer.bias)\n","                    conv_layer.bias.data.copy_(conv_b)\n","                    ptr += num_b\n","                # Load conv. weights\n","                num_w = conv_layer.weight.numel()\n","                conv_w = torch.from_numpy(weights[ptr : ptr + num_w]).view_as(conv_layer.weight)\n","                conv_layer.weight.data.copy_(conv_w)\n","                ptr += num_w\n","\n","    def save_darknet_weights(self, path, cutoff=-1):\n","        \"\"\"\n","            @:param path    - path of the new weights file\n","            @:param cutoff  - save layers between 0 and cutoff (cutoff = -1 -> all are saved)\n","        \"\"\"\n","        fp = open(path, \"wb\")\n","        self.header_info[3] = self.seen\n","        self.header_info.tofile(fp)\n","\n","        # Iterate through layers\n","        for i, (module_def, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):\n","            if module_def[\"type\"] == \"convolutional\":\n","                conv_layer = module[0]\n","                # If batch norm, load bn first\n","                if module_def[\"batch_normalize\"]:\n","                    bn_layer = module[1]\n","                    bn_layer.bias.data.cpu().numpy().tofile(fp)\n","                    bn_layer.weight.data.cpu().numpy().tofile(fp)\n","                    bn_layer.running_mean.data.cpu().numpy().tofile(fp)\n","                    bn_layer.running_var.data.cpu().numpy().tofile(fp)\n","                # Load conv bias\n","                else:\n","                    conv_layer.bias.data.cpu().numpy().tofile(fp)\n","                # Load conv weights\n","                conv_layer.weight.data.cpu().numpy().tofile(fp)\n","\n","        fp.close()"],"metadata":{"id":"6s1CLCzqKMjq","executionInfo":{"status":"ok","timestamp":1666718437309,"user_tz":-540,"elapsed":264,"user":{"displayName":"‍최인훈[학생](소프트웨어융합대학 컴퓨터공학과)","userId":"02055695527872939247"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# Train\n","\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","\n","epochs = 100\n","batch_size = 16\n","gradient_accumulations = 2\n","model_def = 'config/yolov3.cfg'\n","data_config = 'config/nut.data'\n","pretrained_weights = None\n","# pretrained_weights = \"pretrained_100.pth\"\n","n_cpu = 4\n","img_size = 416\n","checkpoint_interval = 1\n","compute_map = False\n","multiscale_training = True\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print('device: ', device)\n","\n","os.makedirs(\"output\", exist_ok=True)\n","os.makedirs(\"checkpoints_nut\", exist_ok=True)\n","\n","# Get data configuration\n","data_config = parse_data_config(data_config)\n","train_path = data_config[\"train\"]\n","valid_path = data_config[\"valid\"]\n","class_names = load_classes(data_config[\"names\"])\n","\n","# Initiate model\n","model = Darknet(model_def).to(device)\n","model.apply(weights_init_normal)\n","\n","\n","# If specified we start from checkpoint\n","if pretrained_weights:\n","    if pretrained_weights.endswith(\".pth\"):\n","        model.load_state_dict(torch.load(pretrained_weights))\n","    else:\n","        model.load_darknet_weights(pretrained_weights)\n","\n","model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n","params = sum([np.prod(p.size()) for p in model_parameters])\n","print('Params: ', params)\n","# Get dataloader\n","dataset = ListDataset(train_path, augment=True, multiscale=multiscale_training)\n","dataloader = torch.utils.data.DataLoader(\n","    dataset,\n","    batch_size=batch_size,\n","    shuffle=False,\n","    num_workers=n_cpu,\n","    pin_memory=True,\n","    collate_fn=dataset.collate_fn,\n",")\n","\n","optimizer = torch.optim.Adam(model.parameters())\n","\n","metrics = [\n","    \"grid_size\",\n","    \"loss\",\n","    \"x\",\n","    \"y\",\n","    \"w\",\n","    \"h\",\n","    \"conf\",\n","    \"cls\",\n","    \"cls_acc\",\n","    \"recall50\",\n","    \"recall75\",\n","    \"precision\",\n","    \"conf_obj\",\n","    \"conf_noobj\",\n","]\n","\n","for epoch in range(epochs):\n","    model.train()\n","    warnings.filterwarnings('ignore', category=UserWarning)\n","    start_time = time.time()\n","    for batch_i, (_, imgs, targets) in enumerate(dataloader):\n","        batches_done = len(dataloader) * epoch + batch_i\n","\n","        imgs = Variable(imgs.to(device))\n","        targets = Variable(targets.to(device), requires_grad=False)\n","\n","        loss, outputs = model(imgs, targets)\n","        loss.backward()\n","\n","        if batches_done % gradient_accumulations:\n","            # Accumulates gradient before each step\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # ----------------\n","        #   Log progress\n","        # ----------------\n","\n","        log_str = \"\\n---- [Epoch %d/%d, Batch %d/%d] ----\\n\" % (epoch, epochs, batch_i, len(dataloader))\n","\n","        metric_table = [[\"Metrics\", *[f\"YOLO Layer {i}\" for i in range(len(model.yolo_layers))]]]\n","\n","        # Log metrics at each YOLO layer\n","        for i, metric in enumerate(metrics):\n","            formats = {m: \"%.6f\" for m in metrics}\n","            formats[\"grid_size\"] = \"%2d\"\n","            formats[\"cls_acc\"] = \"%.2f%%\"\n","            row_metrics = [formats[metric] % yolo.metrics.get(metric, 0) for yolo in model.yolo_layers]\n","            metric_table += [[metric, *row_metrics]]\n","\n","            # Tensorboard logging\n","            tensorboard_log = []\n","            for j, yolo in enumerate(model.yolo_layers):\n","                for name, metric in yolo.metrics.items():\n","                    if name != \"grid_size\":\n","                        tensorboard_log += [(f\"{name}_{j+1}\", metric)]\n","            tensorboard_log += [(\"loss\", loss.item())]\n","            # logger.list_of_scalars_summary(tensorboard_log, batches_done)\n","\n","        log_str += AsciiTable(metric_table).table\n","        log_str += f\"\\nTotal loss {loss.item()}\"\n","\n","        # Determine approximate time left for epoch\n","        epoch_batches_left = len(dataloader) - (batch_i + 1)\n","        time_left = datetime.timedelta(seconds=epoch_batches_left * (time.time() - start_time) / (batch_i + 1))\n","        log_str += f\"\\n---- ETA {time_left}\"\n","\n","        print(log_str)\n","\n","        model.seen += imgs.size(0)\n","\n","\n","    if epoch % checkpoint_interval == 0:\n","        torch.save(model.state_dict(), f\"checkpoints_nut/nut_%d.pth\" % (epoch))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":462},"id":"qxfRzm3vKRdV","executionInfo":{"status":"error","timestamp":1666719789347,"user_tz":-540,"elapsed":3735,"user":{"displayName":"‍최인훈[학생](소프트웨어융합대학 컴퓨터공학과)","userId":"02055695527872939247"}},"outputId":"579154f1-bcc7-4a1a-9a3e-f11ee7e4da40"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["device:  cuda\n","Params:  61599124\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-61615c3001c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-26-395005b811e4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, targets)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmodule_def\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"shortcut\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# shortcut없음\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mlayer_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_def\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"from\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmodule_def\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"yolo\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 14.76 GiB total capacity; 13.32 GiB already allocated; 15.75 MiB free; 13.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]}]}